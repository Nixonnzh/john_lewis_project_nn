# Introduction 
This pipeline aims to transform raw JLP data by ingesting customer survey data, transforming and cleans it, and serves as a platform for other teams to connect data visualisation tools, creating key insights.

The pipeline is split into two sections.

1. Infrastructure creation
2. Data ingestion, cleaning, transformation

# Getting Started
This pipeline requires python 3.7 or later, and Snowflake Python Connector which can be installed with the below.
```
pip install snowflake-connector-python
```
This pipeline assumes the user has a snowflake account, set-up with the following configureable configuration:
- A database called "JLP_BPS"
- A warehouse called 'BIG_WAREHOUSE'
- A schema called "INITIAL_SCHEMA"
- An account with ACCOUNTADMIN access

The pipeline requires two input files, one source dataset .csv and one label description .csv.
## Part 1: Infrastructure Creation

The jlp_infrastructure module is used to create the required tables and connections using column headers from the input files.

The sql_load_labels_on_sf function contains configuration details needed to load the data in, these can be adapated where needed:

`user='jlp_testrun'` The name of the user with required permissions.

`password='Kubrick123'` User's password (Demonstration only).

`account='ed81217.uk-south.azure' ` The account used for loading data.

`warehouse='BIG_WAREHOUSE'` Name of the warehouse used.

`database='JLP_BPS'` Name of Database used.

`schema='initial_schema'` Name of Schema used.

The snowflake infrastructure can be generated by running `jlp_infrastructure.py` as below, this will take around 10 minutes. This file runs the required SQL commands in `jlp_infrastructure_2.sql.`
```
python jlp_infrastructure.py
```
## Part 2: Data ingestion, cleaning, transformation
The jlp_load module is used to load the two input .csv files into the snowflake schema.

The Snowflake Connector is used again with the same configuration details as above which can be adjusted where necessary.

```
python jlp_load.py
```
